{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600220471158",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "\n",
    "# Import to grab popular ML datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Grab MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Datasets grabbed by sklearn often have a similar dictionary strucutre containing\n",
    "# 1. DESCR key describing the dataset\n",
    "# 2. data key containing an array with one row per instance and one column per feature\n",
    "# 3. target key containing an array with the labels\n",
    "\n",
    "# Display arrays\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# 70,000 images with 784 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot out image (28 x 28) into an array with matplotlib\n",
    "some_digit = X[0]\n",
    "some_digital_image = some_digit.reshape(28,28)\n",
    "\n",
    "plt.imshow(some_digital_image, cmap=mpl.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.show\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast y string variables into int variables\n",
    "import numpy as np\n",
    "y = y.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test/train sets\n",
    "# Note: All sets are already shuffled\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classifier for 5\n",
    "y_train_5 = (y_train == 5) # True for 5, false for non-5\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "# Training classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# SGDCLassifier model\n",
    "# Note: very good at online learning\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "# Fit model\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict first image in array representing 5\n",
    "sgd_clf.predict([some_digit])\n",
    "\n",
    "# Correctly predicts the number 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate performance of SGDClassifier model at current time\n",
    "# It is more difficult to predict accuracy of classifier models than regressor models\n",
    "# Many different evaluation measures will be shown\n",
    "\n",
    "# Measuring accuracy using cross-validation\n",
    "# Similarly used in Chp_2 project\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# In order to gain more control in this cross-validation process the following code implements\n",
    "# cross-validation manually\n",
    "\n",
    "# Random_state ensures repeatable outputs. 42 is generally used\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "\n",
    "# StratifiedKFold performs stratified sampling (expl. chp. 2)\n",
    "# at each iteration the code creates a clone of the classifier, trains the clone on the training folds, and makes predictions on the\n",
    "# test fold\n",
    "# then it counts the number of correct predictions and outputs the ratio of correct predictions\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train_5[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_5[test_index]\n",
    "\n",
    "    # Output ratio of correct predictions\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred)) # 0.95035, 0.96035, 0.9604"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_score to gauge accuracy (same as code above)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# See Chp 2 for further information\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "# 0.95035, 0.96035, 0.9604"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumb classifier that will look at every image in not-5 category\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self,X,y=None):\n",
    "        pass\n",
    "    def predict(self,X):\n",
    "        return np.zeros((len(X),1), dtype=bool)\n",
    "\n",
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "# returns 0.91125, 0.90855, 0.90915\n",
    "# About 10% of the images are 5. So if you guess that an image is 'not-5' you will be right about 90% of the time\n",
    "# Accuracy is not generally a good indicator for classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix -- more ideal method of calculating classifier performance\n",
    "# Essentially counts the number of times instances of class A are classified as class B\n",
    "# e.g knowing the amount of times the classifier confused images of 5s with 3s\n",
    "\n",
    "# Initial step -- calculate set of predictions\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# cross_val_predict is similar to cross_val_score\n",
    "# performs k fold cross-validation but calculates predictions instead of scores\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix creation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred) # returns 53892, 687 (next row) 1891, 3530\n",
    "# each row represents an actual class, while each column represents a predicted class\n",
    "# Row 1: Correctly guessed non-5's, then false positives (incorrectly guessed as 5)\n",
    "# Row 2: False negatives (wrongly classified as non5s), then true positives (correctly classified as 5)\n",
    "# A perfect model would only have true negatives and true positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perfect_predictions = y_train_5 # pretending that the model has reached perfection\n",
    "confusion_matrix(y_train_5, y_train_perfect_predictions) # Confusion matrix based on a 'perfect model' with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Precision calculations\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(precision_score(y_train_5, y_train_pred)) # Precision score, usually used in conjunction with recall score\n",
    "print(recall_score(y_train_5, y_train_pred)) # recall score\n",
    "\n",
    "# Precision score:  TP / (TP + FP)\n",
    "# Recall score: TP / (TP + FN)\n",
    "\n",
    "# Prints 0.83708 ...\n",
    "# Prints 0.651171 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The F1 score is the harmonic mean between the precision score and recall score\n",
    "\n",
    "# F1 score:\n",
    "# 2/(1/precision + 1/recall)\n",
    "# Code for F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The decision threshhold for the classifier cannot be changed directly\n",
    "# However, the decision scores that it uses to make predictions can be accessed\n",
    "\n",
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "print(y_scores) # Print 2164.2203 ...\n",
    "\n",
    "threshhold = 0\n",
    "y_some_digit_pred = (y_scores > threshhold)\n",
    "print(y_some_digit_pred) # Print array [ True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshhold = 8000 \n",
    "y_some_digit_pred = (y_scores > threshhold) # Boolean value for testing threshold vs score\n",
    "# when score is higher than threshhold it is true\n",
    "print(y_some_digit_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the right decision score to use across all instances\n",
    "# not just [0]\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\") # return decision scores\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, threshholds = precision_recall_curve(y_train_5, y_scores) # Compue precision and recall for all possible threshholds\n",
    "\n",
    "# Function for plotting precision and recall\n",
    "# Graph can be used for precision / recall tradeoff calculations\n",
    "def plot_precision_recall_vs_threshhold(precisions,recalls,threshholds):\n",
    "    plt.plot(threshholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(threshholds,recalls[:-1], \"g-\", label=\"Recall\")\n",
    "\n",
    "plot_precision_recall_vs_threshhold(precisions,recalls,threshholds) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With a goal of 90% precision, the exact threshhold value can be calculated\n",
    "threshhold_90_precision = threshholds[np.argmax(precisions >= 0.90)]\n",
    "print(threshhold_90_precision) # prints 3370.019\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_90 = (y_scores >= threshhold_90_precision) # create boolean\n",
    "\n",
    "print(precision_score(y_train_5, y_train_pred_90)) # precision using boolean -- print 0.9000345\n",
    "print(recall_score(y_train_5, y_train_pred_90)) # recall using boolean -- print 0.479985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve from sklearn.metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_roc_curve(fpr,tpr,label=None):\n",
    "    plt.plot(fpr,tpr,linewidth=2,label=label)\n",
    "    plt.plot([0,1],[0,1],'k--') # dashed diagonal\n",
    "\n",
    "plot_roc_curve(fpr,tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# AUC (area under the curve)\n",
    "# perfect classifier has AUC == 1 && ROC == 1\n",
    "# completely random classifier has AUC == 0.5 && ROC == 0.5\n",
    "roc_auc_score(y_train_5, y_scores) # print 0.960493 ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training RandomForestClassifier to compare ROC and ROC/AUC to SGDClassifier ROC ROC/AUC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42) # Create model\n",
    "y_probas_forest = cross_val_predict(forest_clf,X_train,y_train_5,cv=3,method=\"predict_proba\") # Gather probabilities\n",
    "\n",
    "y_scores_forest = y_probas_forest[:,1] # Gather scores\n",
    "fpr_forest,tpr_forest,threshholds_forest = roc_curve(y_train_5,y_scores_forest) # Info for plotting ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SGD to Random Forest\n",
    "plt.plot(fpr,tpr,\"b:\",label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest,tpr_forest,\"Random Forest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_5,y_scores_forest) # print 0.998343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3) # prediction variable\n",
    "print(precision_score(y_train_5, y_train_pred_forest)) # prints 0.990508\n",
    "print(recall_score(y_train_5, y_train_pred_forest)) # prints 0.86626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multiclass classifiers -- e.g comparing more than two classes. (as opposed to binary classifiers that compare 2 options)\n",
    "# multiclass operations can be conducted by using multiple binary class classifiers (one versus all strategy OvA / or one verses one OvO)\n",
    "'''\n",
    "Scikit-Learn detects when you try to use a binary classification algorithm for a multi‚Äê\n",
    "class classification task, and it automatically runs OvA (except for SVM classifiers for\n",
    "which it uses OvO).\n",
    "'''\n",
    "\n",
    "# Testing scikit-learn's detection operation -- trains SGDClassifier on the training set using the\n",
    "# the original target classes from 0 to 9 (y_train), instead of 5 v all target classes (y_train_5)\n",
    "sgd_clf.fit(X_train,y_train)\n",
    "sgd_clf.predict([some_digit])\n",
    "\n",
    "# decision_function method -- returns 10 scores per instance instead of just one per instance (trained 10 classifiers)\n",
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "print(some_digit_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Highest score on some_digit_scores corresponds to 5\n",
    "print(np.argmax(some_digit_scores)) # returns highest prediction for some_digit_scores e.g [some_digit]\n",
    "\n",
    "print(sgd_clf.classes_) # all classes\n",
    "\n",
    "print(sgd_clf.classes_[5]) # fifth class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[5]\n45\n"
    }
   ],
   "source": [
    "# To change strategies using scikit-learn (OvA or OvO) you can use the OneVsOneClassifier or OneVsRestCLassifier classes\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42)) # Create model using OvO/SGDClassifier\n",
    "ovo_clf.fit(X_train,y_train) # Fit model\n",
    "print(ovo_clf.predict([some_digit])) # predict [some_digit]\n",
    "print(len(ovo_clf.estimators_)) # Print out length of all estimators (in this case 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([5], dtype=uint8)"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# RandomForestClassifier model using OvO\n",
    "forest_clf.fit(X_train,y_train)\n",
    "forest_clf.predict([some_digit])\n",
    "\n",
    "'''\n",
    "This time Scikit-Learn did not have to run OvA or OvO because Random Forest\n",
    "classifiers can directly classify instances into multiple classes. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}